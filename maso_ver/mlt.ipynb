{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습셋, 테스트 셋 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:02:53.505035Z",
     "start_time": "2018-01-06T15:02:53.368273Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['MXNET_ENGINE_TYPE'] = 'NaiveEngine'\n",
    "#os.environ['MXNET_CUDNN_AUTOTUNE_DEFAULT'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:02:54.006544Z",
     "start_time": "2018-01-06T15:02:53.993993Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#mecab = Mecab()\n",
    "from konlpy.tag import Mecab\n",
    "from stemming.porter2 import stem\n",
    "mecab = Mecab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:29.313078Z",
     "start_time": "2018-01-06T15:02:54.255756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "##어근 추출 \n",
    "def make_list(fname, l= 'ko'):\n",
    "    spaced = []\n",
    "    with open(fname) as f:\n",
    "        if l == 'ko':\n",
    "            spaced = [['START', ] + mecab.morphs(i.strip()) + ['END', ] for i in f.readlines()]\n",
    "        else:\n",
    "            spaced = [['START', ] + [stem(j) for j in i.lower().split()] + ['END', ] for i in f.readlines()]\n",
    "    return(spaced)\n",
    "\n",
    "\n",
    "train_ko = make_list('../korean_parallel_corpora/korean-english-v1/korean-english-park.train.ko',l='ko')\n",
    "len(train_ko)\n",
    "test_ko = make_list('../korean_parallel_corpora/korean-english-v1/korean-english-park.test.ko',l='ko')\n",
    "len(test_ko)\n",
    "\n",
    "\n",
    "train_en = make_list('../korean_parallel_corpora/korean-english-v1/korean-english-park.train.en',l='en')\n",
    "len(train_en)\n",
    "test_en = make_list('../korean_parallel_corpora/korean-english-v1/korean-english-park.test.en',l='en')\n",
    "len(test_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:29.315380Z",
     "start_time": "2018-01-06T15:03:29.313967Z"
    }
   },
   "outputs": [],
   "source": [
    "max_seq_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:29.700392Z",
     "start_time": "2018-01-06T15:03:29.617035Z"
    }
   },
   "outputs": [],
   "source": [
    "#한글 문자 영어 문장 두 문장 쌍 중 하나의 문장이라도 max_seq_len을 초과하는지 확인 후 제거 \n",
    "#완벽한 문장으로 학습을 하기 위함 \n",
    "\n",
    "tr_idx_ko = [idx  for idx, i in enumerate(train_ko) if len(i) <= max_seq_len]\n",
    "tr_idx_en = [idx  for idx, i in enumerate(train_en) if len(i) <= max_seq_len]\n",
    "\n",
    "tr = np.intersect1d(tr_idx_ko, tr_idx_en)\n",
    "tr = set(tr)\n",
    "\n",
    "te_idx_ko = [idx  for idx, i in enumerate(test_ko) if len(i) <= max_seq_len]\n",
    "te_idx_en = [idx  for idx, i in enumerate(test_en) if len(i) <= max_seq_len]\n",
    "\n",
    "te = np.intersect1d(te_idx_ko, te_idx_en)\n",
    "te = set(te)\n",
    "\n",
    "train_ko_f = [i  for idx, i in enumerate(train_ko) if idx in tr] \n",
    "train_en_f = [i  for idx, i in enumerate(train_en) if idx in tr] \n",
    "\n",
    "test_ko_f = [i  for idx, i in enumerate(test_ko) if idx in te] \n",
    "test_en_f = [i  for idx, i in enumerate(test_en) if idx in te] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:30.460187Z",
     "start_time": "2018-01-06T15:03:29.773902Z"
    }
   },
   "outputs": [],
   "source": [
    "from  embedding_maker import *\n",
    "w2idx, idx_2w = load_vocab(\"../ko_en.dic\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:30.462380Z",
     "start_time": "2018-01-06T15:03:30.461095Z"
    }
   },
   "outputs": [],
   "source": [
    "max_seq = max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:30.560254Z",
     "start_time": "2018-01-06T15:03:30.463126Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, maxlen=None, dtype='int32',\n",
    "                  padding='pre', truncating='pre', value=0.):\n",
    "    if not hasattr(sequences, '__len__'):\n",
    "        raise ValueError('`sequences` must be iterable.')\n",
    "    lengths = []\n",
    "    for x in sequences:\n",
    "        if not hasattr(x, '__len__'):\n",
    "            raise ValueError('`sequences` must be a list of iterables. '\n",
    "                             'Found non-iterable: ' + str(x))\n",
    "        lengths.append(len(x))\n",
    "\n",
    "    num_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((num_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if not len(s):\n",
    "            continue  # empty list/array was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:30.674887Z",
     "start_time": "2018-01-06T15:03:30.561595Z"
    }
   },
   "outputs": [],
   "source": [
    "def encoding_and_padding(corp_list, dic, max_seq=50):\n",
    "    coding_seq = [ [dic.get(j, dic['__ETC__']) for j in i]  for i in corp_list ]\n",
    "    return(pad_sequences(coding_seq, maxlen=max_seq, padding='post', truncating='post',value=dic['__PAD__']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:30.983624Z",
     "start_time": "2018-01-06T15:03:30.675737Z"
    }
   },
   "outputs": [],
   "source": [
    "ko_train_x = encoding_and_padding(train_ko_f, w2idx, max_seq=max_seq)\n",
    "ko_test_x = encoding_and_padding(test_ko_f, w2idx, max_seq=max_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:31.088425Z",
     "start_time": "2018-01-06T15:03:30.984514Z"
    }
   },
   "outputs": [],
   "source": [
    "en_train_x = encoding_and_padding(train_en_f, w2idx, max_seq=max_seq)\n",
    "en_test_x = encoding_and_padding(test_en_f, w2idx, max_seq=max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:31.298319Z",
     "start_time": "2018-01-06T15:03:31.089240Z"
    }
   },
   "outputs": [],
   "source": [
    "train_en_lag = [ i[1:] for i in train_en_f]\n",
    "test_en_lag = [ i[1:] for i in test_en_f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:31.512286Z",
     "start_time": "2018-01-06T15:03:31.299278Z"
    }
   },
   "outputs": [],
   "source": [
    "en_train_y = encoding_and_padding(train_en_lag, w2idx, max_seq=max_seq)\n",
    "en_test_y = encoding_and_padding(test_en_lag, w2idx, max_seq=max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:31.545779Z",
     "start_time": "2018-01-06T15:03:31.513117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21002, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max_seq \n",
    "vocab_size = len(w2idx)\n",
    "n_hidden = 256\n",
    "embed_dim = 50\n",
    "embed_weights  = load_embedding(\"../ko_en.np\")\n",
    "embed_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gluon Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:31.913580Z",
     "start_time": "2018-01-06T15:03:31.546556Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mxnet import nd as  F\n",
    "import mxnet as mx\n",
    "from  mxnet import gluon\n",
    "from mxnet.gluon import nn, rnn\n",
    "import mxnet.autograd as autograd\n",
    "from mxnet import nd as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:32.249451Z",
     "start_time": "2018-01-06T15:03:31.914574Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class korean_english_translator(gluon.HybridBlock):\n",
    "    \n",
    "    def __init__(self, n_hidden, vocab_size, embed_dim, max_seq_length,attention=False ,**kwargs):\n",
    "        super(korean_english_translator,self).__init__(**kwargs)\n",
    "        #입력 시퀀스 길이\n",
    "        self.in_seq_len = max_seq_length\n",
    "        #출력 시퀀스 길이 \n",
    "        self.out_seq_len = max_seq_length\n",
    "        # LSTM의 hidden 개수 \n",
    "        self.n_hidden = n_hidden\n",
    "        #고유문자개수 \n",
    "        self.vocab_size = vocab_size\n",
    "        #max_seq_length\n",
    "        self.max_seq_length = max_seq_length\n",
    "        #임베딩 차원수 \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        \n",
    "        self.attention = attention\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(input_dim=vocab_size, output_dim=embed_dim, dtype=\"float16\")\n",
    "\n",
    "            \n",
    "            self.encoder= rnn.BidirectionalCell(rnn.GRUCell(hidden_size=int(n_hidden/2)),  \n",
    "                                                rnn.GRUCell(hidden_size=int(n_hidden/2)))\n",
    "            self.decoder = rnn.GRUCell(hidden_size=n_hidden)\n",
    "            self.batchnorm = nn.BatchNorm(axis=2)\n",
    "            #flatten을 false로 할 경우 마지막 차원에 fully connected가 적용된다. \n",
    "            #(N, max_seq_len,  vacab size)\n",
    "            self.dense = nn.Dense(self.vocab_size,flatten=False)\n",
    "            if self.attention:\n",
    "                self.attdense = nn.Dense(self.max_seq_length, flatten=False, activation='relu')\n",
    "                self.attn_combine = nn.Dense( self.embed_dim, flatten=False, activation='relu')\n",
    "        #self.embedding.weight.set_data(embed_weights)\n",
    "        #self.embedding.collect_params().setattr('grad_req', 'null')\n",
    "            \n",
    "    def hybrid_forward(self, F, inputs, outputs):\n",
    "        \"\"\"\n",
    "        학습 코드 \n",
    "        \"\"\"\n",
    "        #encoder LSTM\n",
    "        embeddinged_in = F.cast(self.embedding(inputs), dtype='float32')\n",
    "\n",
    "        enout, (next_l, next_r) = self.encoder.unroll(inputs=embeddinged_in, length=self.in_seq_len, \n",
    "                                                          merge_outputs=True)   \n",
    "        next_h = F.concat(next_l, next_r, dim=1)\n",
    "        embeddinged_out = F.cast(self.embedding(outputs),dtype='float32')\n",
    "        \n",
    "        #decoder LSTM with attention\n",
    "        for i in range(self.out_seq_len):\n",
    "            #out_seq_len 길이만큼 LSTMcell을 unroll하면서 출력값을 적재한다. \n",
    "            p_outputs = F.slice_axis(embeddinged_out, axis=1, begin=i, end=i+1)\n",
    "            p_outputs = F.reshape(p_outputs, (-1, self.embed_dim))\n",
    "            # p_outputs = embeddinged_out[:,i,:]\n",
    "            # 위와 같이 진행한 이유는 hybridize를 위함 \n",
    "            if self.attention:\n",
    "                print(p_outputs.shape)\n",
    "                p_outputs, _ = self.apply_attention(F=F, inputs=p_outputs, hidden=next_h, encoder_outputs=enout)\n",
    "                print(p_outputs.shape)\n",
    "            deout, (next_h,) = self.decoder(p_outputs, [next_h,] )\n",
    "            if i == 0:\n",
    "                deouts = deout\n",
    "            else:\n",
    "                deouts = F.concat(deouts, deout, dim=1)\n",
    "        #2dim -> 3dim 으로 reshape \n",
    "        deouts_orig = F.reshape(deouts, (-1, self.out_seq_len, self.n_hidden))\n",
    "        \n",
    "        deouts = self.batchnorm(deouts_orig)\n",
    "        deouts_fc = self.dense(deouts)\n",
    "        return(deouts_fc)\n",
    "    \n",
    "    def apply_attention(self, F, inputs, hidden, encoder_outputs):\n",
    "        #inputs : decoder input의미\n",
    "        concated = F.concat(inputs, hidden, dim=1)\n",
    "        #(,max_seq_length) : max_seq_length 개별 시퀀스의 중요도  \n",
    "        attn_weights = F.softmax(self.attdense(concated), axis=1)\n",
    "        # (N,max_seq_length,n_hidden) x (N,max_seq_length,1) = (N, max_seq_length, n_hidden)\n",
    "        #attn_weigths 가중치를 인코더 출력값에 곱해줌\n",
    "        w_encoder_outputs = F.broadcast_mul(encoder_outputs, attn_weights.expand_dims(2))\n",
    "        #(N, embed_dim), (N, max_seq_length * n_hidden) = (N, ...)\n",
    "        output = F.concat(inputs.flatten(), w_encoder_outputs.flatten(), dim=1)\n",
    "        #(N, vocab_size)\n",
    "        output = self.attn_combine(output)\n",
    "        #attention weight은 시각화를 위해 뽑아둔다. \n",
    "        return(output, attn_weights)\n",
    "    \n",
    "    def calulation(self, input_str, ko_dict, en_dict, en_rev_dict, ctx=mx.gpu(0)):\n",
    "        \"\"\"\n",
    "        inference 코드 \n",
    "        \"\"\"\n",
    "        #앞뒤에 START,END 코드 추가 \n",
    "        input_str = [['START', ] + mecab.morphs(input_str.strip()) + ['END', ],]\n",
    "        X = encoding_and_padding(input_str, ko_dict, max_seq=self.max_seq_length)\n",
    "        #string to embed \n",
    "        X = F.cast(self.embedding(F.array(X, ctx=ctx)), dtype='float32')\n",
    "\n",
    "        #인코더 출력값을 도출한다. \n",
    "        #enout_base, (next_h,) = self.encoder.unroll(inputs=X, length=self.in_seq_len, merge_outputs=True)\n",
    "        enout, (next_l, next_r) = self.encoder.unroll(inputs=X, length=self.in_seq_len, \n",
    "                                                          merge_outputs=True)\n",
    "        next_h = F.concat(next_l, next_r, dim=1)\n",
    "        #디코더의 초기 입력값으로 넣을 'START'를 임베딩한다.\n",
    "        Y_init = F.array([[en_dict['START'],],], ctx=ctx)\n",
    "        Y_init = F.cast(self.embedding(Y_init),dtype='float32')\n",
    "        deout = Y_init[:,0,:]\n",
    "        \n",
    "        #출력 시퀀스 길이만큼 순회 \n",
    "        for i in range(self.out_seq_len):\n",
    "            if self.attention:\n",
    "                #print(deout.shape)\n",
    "                deout, att_weight = self.apply_attention(F=F, inputs=deout, hidden=next_h, encoder_outputs=enout)\n",
    "                if i == 0:\n",
    "                    att_weights = att_weight\n",
    "                else:\n",
    "                    att_weights = F.concat(att_weights,att_weight,dim=0)\n",
    "            deout, (next_h, ) = self.decoder(deout, [next_h, ])\n",
    "            #batchnorm을 적용하기 위해 차원 증가/원복 \n",
    "            deout = F.expand_dims(deout,axis=1)\n",
    "            deout = self.batchnorm(deout)\n",
    "            #reduce dim\n",
    "            deout = deout[:,0,:]\n",
    "            #'START'의 다음 시퀀스 출력값도출 \n",
    "            deout_sm = self.dense(deout)\n",
    "            #print(deout_sm.shape)\n",
    "            deout = F.one_hot(F.argmax(F.softmax(deout_sm, axis=1), axis=1), depth=self.vocab_size)\n",
    "            #print(deout.shape)\n",
    "            #decoder에 들어갈 수 있는 형태로 변환(임베딩 적용 및 차원 맞춤)\n",
    "            deout = F.argmax(deout, axis=1)\n",
    "            deout = F.expand_dims(deout, axis=0)\n",
    "            deout = F.cast(self.embedding(deout)[:,0,:],dtype='float32')\n",
    "            gen_char  = en_rev_dict[F.argmax(deout_sm, axis=1).asnumpy()[0].astype('int')]\n",
    "            if gen_char == '__PAD__' or gen_char == 'END':\n",
    "                break\n",
    "            else: \n",
    "                if i == 0:\n",
    "                    ret_seq = [gen_char, ]\n",
    "                else:\n",
    "                    ret_seq += [gen_char, ]\n",
    "        return(\" \".join(ret_seq), att_weights)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델링 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:32.412800Z",
     "start_time": "2018-01-06T15:03:32.250313Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_set = gluon.data.ArrayDataset(ko_train_x, en_train_x, en_train_y)\n",
    "tr_data_iterator = gluon.data.DataLoader(tr_set, batch_size=100, shuffle=True)\n",
    "\n",
    "te_set =gluon.data.ArrayDataset(ko_test_x, en_test_x, en_test_y)\n",
    "te_data_iterator = gluon.data.DataLoader(te_set, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for l, (i,j,k) in enumerate(tr_data_iterator):\n",
    "    i = i\n",
    "    j = j\n",
    "    k = k\n",
    "    break\n",
    "\n",
    "x = F.random.normal(0, 1, shape=(1,30))\n",
    "y = F.random.normal(0, 1, shape=(1,30))\n",
    "\n",
    "\n",
    "model = korean_english_translator(n_hidden, vocab_size, embed_dim, max_seq_length,attention=True)\n",
    "\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=mx.cpu(0))\n",
    "\n",
    "model(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:32.514981Z",
     "start_time": "2018-01-06T15:03:32.417234Z"
    }
   },
   "outputs": [],
   "source": [
    "GPU_COUNT = 2\n",
    "ctx= [mx.gpu(i) for i in range(GPU_COUNT)]\n",
    "\n",
    "def model_init():\n",
    "    #모형 인스턴스 생성 및 트래이너, loss 정의 \n",
    "    #n_hidden, vocab_size, embed_dim, max_seq_length\n",
    "    model = korean_english_translator(n_hidden, vocab_size, embed_dim, max_seq_length,attention=True)\n",
    "    model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "    model.embedding.weight.set_data(embed_weights)\n",
    "         \n",
    "    trainer = gluon.Trainer(model.collect_params(), 'rmsprop', kvstore='local') \n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss(axis = 2)\n",
    "    return(model, loss, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:32.803852Z",
     "start_time": "2018-01-06T15:03:32.727468Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx=ctx):\n",
    "    test_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(data_iter):\n",
    "        x_data_l = gluon.utils.split_and_load(x_data, ctx,  even_split=False)\n",
    "        y_data_l = gluon.utils.split_and_load(y_data, ctx,  even_split=False)\n",
    "        z_data_l = gluon.utils.split_and_load(z_data, ctx,  even_split=False)\n",
    "        with autograd.predict_mode():\n",
    "            losses = [loss_obj(model(x, y), z) for x, y, z in zip(x_data_l, y_data_l, z_data_l)]\n",
    "        curr_loss = (mx.nd.mean( losses[0] ).asscalar() + mx.nd.mean( losses[1] ).asscalar())/2\n",
    "        test_loss.append(curr_loss)\n",
    "    return(np.mean(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:32.925748Z",
     "start_time": "2018-01-06T15:03:32.808422Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(epochs, tr_data_iterator, model, loss, trainer, ctx= [mx.gpu(i) for i in range(GPU_COUNT)], mdl_desc=\"k2e_model\"):\n",
    "    ### 학습 코드 \n",
    "    tot_test_loss = []\n",
    "    tot_train_loss = []\n",
    "    for e in range(epochs):\n",
    "        tic = time.time()\n",
    "        # Decay learning rate.\n",
    "        if e > 1:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.5)\n",
    "        train_loss = []\n",
    "        for i, (x_data, y_data, z_data) in enumerate(tqdm(tr_data_iterator)):\n",
    "            #x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "            #y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "            #z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "            \n",
    "            x_data_l = gluon.utils.split_and_load(x_data, ctx, even_split=False)\n",
    "            y_data_l = gluon.utils.split_and_load(y_data, ctx, even_split=False)\n",
    "            z_data_l = gluon.utils.split_and_load(z_data, ctx, even_split=False)\n",
    "            \n",
    "            with autograd.record():\n",
    "                losses = [loss(model(x, y), z) for x, y, z in zip(x_data_l, y_data_l, z_data_l)]\n",
    "            for l in losses:\n",
    "                l.backward()\n",
    "            trainer.step(x_data.shape[0])\n",
    "            curr_loss = (mx.nd.mean( losses[0] ).asscalar() + mx.nd.mean( losses[1] ).asscalar())/2\n",
    "            train_loss.append(curr_loss)\n",
    "            mx.nd.waitall()\n",
    "        #caculate test loss\n",
    "        test_loss = calculate_loss(model, te_data_iterator, loss_obj = loss, ctx=ctx) \n",
    "        print('[Epoch %d] time cost: %f'%(e, time.time()-tic))\n",
    "        print(\"Epoch %s. Train Loss: %s, Test Loss : %s\" % (e, np.mean(train_loss), test_loss))    \n",
    "        tot_test_loss.append(test_loss)\n",
    "        tot_train_loss.append(np.mean(train_loss))\n",
    "        model.save_params(\"{}_{}.params\".format(mdl_desc, e))\n",
    "    return(tot_test_loss, tot_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:34.858204Z",
     "start_time": "2018-01-06T15:03:32.926941Z"
    }
   },
   "outputs": [],
   "source": [
    "model, loss, trainer = model_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T15:03:35.178921Z",
     "start_time": "2018-01-06T15:03:35.087754Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-06T15:07:51.121Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.hybridize()\n",
    "train(10, tr_data_iterator, model, loss, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain \n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', optimizer_params={'learning_rate':1,'clip_gradient':0.99, 'wd':1e-5,})\n",
    "#loss = gluon.loss.SoftmaxCrossEntropyLoss(axis = 2)\n",
    "tr_set = gluon.data.ArrayDataset(ko_train_x, en_train_x, en_train_y)\n",
    "tr_data_iterator = gluon.data.DataLoader(tr_set, batch_size=100, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#epochs = 5\n",
    "#model.hybridize()\n",
    "test_loss, train_loss = train(10, tr_data_iterator, model, loss, trainer,ctx=ctx,mdl_desc='k2e_model_sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T14:23:47.034247Z",
     "start_time": "2018-01-06T14:23:46.890309Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_seq,att_weight  = model.calulation(\"좋은 아침.\", ko_dict=w2idx, en_dict=w2idx, en_rev_dict=idx_2w)\n",
    "\n",
    "eng_seq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
